apiVersion: tekton.dev/v1
kind: Task
metadata:
  name: upload-data-card
spec:
  params:
    - name: payload_body
      type: string
    - name: event_name
      type: string
    - name: key
      type: string
    - name: records
      type: string
    - name: endpoint
      type: string
  steps:
    - env:
        - name: PAYLOAD_BODY
          value: $(params.payload_body)
        - name: EVENT_NAME
          value: $(params.event_name)
        - name: KEY
          value: $(params.key)
        - name: RECORDS
          value: $(params.records)
        - name: ENDPOINT
          value: $(params.endpoint)
        # We included minio creds here for demonstration purposes.
      #This image was built using the included containerfile.
      image: 'quay.io/rh_ee_akugel/datacard-python:1.1'
      name: data-card-upload
      script: |
        #!/usr/bin/env python3

        import boto3
        import botocore
        import os
        from io import BytesIO
        import json
        import botocore.exceptions

        def main():

            BUCKET_NAME = "data-cards"

            aws_access_key_id = os.environ.get("ACCESS_KEY")
            aws_secret_access_key = os.environ.get("SECRET_KEY")

            s3_client = boto3.client(
                service_name="s3",
                aws_access_key_id=aws_access_key_id,
                aws_secret_access_key=aws_secret_access_key,
                endpoint_url="https://minio-api-minio.apps.cluster-497z4.497z4.sandbox2736.opentlc.com",
            )

            # with open("create_event.json", "r") as file:
            #     data = json.load(file)

            event = dict()
            event["eventName"] = os.environ.get("EVENT_NAME")
            event["key"] = os.environ.get("KEY")
            event["records"] = json.loads(os.environ.get("RECORDS"))
            event["endpoint"] = os.environ.get("ENDPOINT")
            # data = json.loads(os.environ.get("payload_body"))

            try:
                # Check if the object already exists
                s3_client.head_object(Bucket=BUCKET_NAME, Key=event["key"])
                print(f"{event['key']}")
                print(f"File {event['key']} already exists in the bucket.")

                # Read the contents of the existing file in S3
                existing_file = s3_client.get_object(Bucket=BUCKET_NAME, Key=event["key"])
                existing_file_content = existing_file["Body"].read().decode("utf-8")
                existing_file_json = json.loads(existing_file_content)
                # Combine both files
                # combined_content = existing_file_content + "\n" + str(data)

                for event_record in existing_file_json["records"]:
                    event["records"].append(event_record)

                json_event = json.dumps(event)
                bytes_data = bytes(str(json_event), "utf-8")
                # Create a file object
                file_obj = BytesIO(bytes_data)

                # Upload the combined content to S3
                s3_client.put_object(Body=file_obj, Bucket=BUCKET_NAME, Key=event["key"])
                print(f"Combined file uploaded to {event['key']} in S3.")

            except botocore.exceptions.ClientError as e:
                print("exception caught", e.response["Error"]["Code"])
                # If the object doesn"t exist, upload the new file
                if e.response["Error"]["Code"] == "404":

                    json_event = json.dumps(event)
                    bytes_data = bytes(str(json_event), "utf-8")
                    # Create a file object
                    file_obj = BytesIO(bytes_data)

                    s3_client.put_object(Body=file_obj, Bucket=BUCKET_NAME, Key=event["key"])
                    print(f"Bucket {BUCKET_NAME} created.")
                    print(f"File {event['key']} uploaded to S3.")

                else:
                    # Reraise other types of errors
                    raise


        if __name__ == "__main__":
            main()
